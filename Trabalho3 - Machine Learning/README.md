# Machine Learning - ImplementaÃ§Ãµes

ImplementaÃ§Ãµes de algoritmos de Machine Learning em Python puro (sem bibliotecas de ML).

## ğŸ“š Algoritmos Implementados

1. **KNN Classificador** - K-Nearest Neighbors para classificaÃ§Ã£o
2. **KNN Regressor** - K-Nearest Neighbors para regressÃ£o
3. **K-Means** - Agrupamento (clustering)
4. **RegressÃ£o LogÃ­stica** - ClassificaÃ§Ã£o binÃ¡ria
5. **Decision Tree** - Ãrvore de DecisÃ£o
6. **Redes Neurais** - Multilayer Perceptron com backpropagation

## ğŸ› ï¸ Bibliotecas Permitidas

- **NumPy** - Para operaÃ§Ãµes matemÃ¡ticas
- **Pandas** - Para manipulaÃ§Ã£o de dados
- **Matplotlib** - Para visualizaÃ§Ãµes
- **Sklearn** - Utilizada APENAS para carregar os datasets (nÃ£o usar os algoritmos!)

## ğŸ“¦ InstalaÃ§Ã£o das DependÃªncias

```bash
pip install numpy pandas matplotlib scikit-learn
```

ou

```bash
pip instal requirements.txt
```

## ğŸš€ Como Executar

Cada algoritmo tem seu prÃ³prio diretÃ³rio com dois arquivos:
- `<algoritmo>.py` - ImplementaÃ§Ã£o da classe
- `teste_<algoritmo>.py` - Exemplo de uso com dataset

Para executar um teste:

```bash
# Exemplo: KNN Classificador
cd "knn_classificador"
python teste_knn_classificador.py

# Exemplo: Redes Neurais
cd "redes neurais"
python teste_rede_neural.py
```

## ğŸ“Š Onde Encontrar Datasets PÃºblicos

### 1. Sklearn Datasets (Mais FÃ¡cil)
```python
from sklearn import datasets

#ClassificaÃ§Ã£o
iris = datasets.load_iris()              #Flores (3 classes)
digits = datasets.load_digits()          #DÃ­gitos 0-9
wine = datasets.load_wine()              #Vinhos
breast_cancer = datasets.load_breast_cancer()  #CÃ¢ncer (binÃ¡rio)

#RegressÃ£o
california = datasets.fetch_california_housing()  #PreÃ§os de casas
diabetes = datasets.load_diabetes()      #ProgressÃ£o de diabetes
```

### 2. UCI Machine Learning Repository
- Site: https://archive.ics.uci.edu/ml/
- Baixe arquivos CSV e carregue com pandas:
```python
import pandas as pd
df = pd.read_csv('dataset.csv')
X = df.drop('target_column', axis=1).values
y = df['target_column'].values
```

### 3. OpenML
- Site: https://www.openml.org/
- Integra com sklearn:
```python
from sklearn.datasets import fetch_openml
data = fetch_openml(name='diabetes', version=1)
```

## ğŸ“ Estrutura dos Arquivos

```
Trabalho3 - Machine Learning/
â”œâ”€â”€ knn_classificador/
â”‚   â”œâ”€â”€ knn_classificador.py
â”‚   â””â”€â”€ teste_knn_classificador.py
â”œâ”€â”€ knn_regressor/
â”‚   â”œâ”€â”€ knn_regressor.py
â”‚   â””â”€â”€ teste_knn_regressor.py
â”œâ”€â”€ kmeans/
â”‚   â”œâ”€â”€ kmeans.py
â”‚   â””â”€â”€ teste_kmeans.py
â”œâ”€â”€ regressao_logistica/
â”‚   â”œâ”€â”€ regressao_logistica.py
â”‚   â””â”€â”€ teste_regressao_logistica.py
â”œâ”€â”€ decision_tree/
â”‚   â”œâ”€â”€ decision_tree.py
â”‚   â””â”€â”€ teste_decision_tree.py
â””â”€â”€ redes neurais/
    â”œâ”€â”€ rede_neural.py
    â””â”€â”€ teste_rede_neural.py
```

## ğŸ¯ CaracterÃ­sticas das ImplementaÃ§Ãµes

### KNN Classificador
- DistÃ¢ncia euclidiana
- Voto majoritÃ¡rio dos K vizinhos
- Teste com dataset Iris

### KNN Regressor
- DistÃ¢ncia euclidiana
- MÃ©dia dos valores dos K vizinhos
- Teste com dataset California Housing

### K-Means
- InicializaÃ§Ã£o aleatÃ³ria
- CritÃ©rio de convergÃªncia
- MÃ©todo do cotovelo para escolher K
- Teste com dataset Iris

### RegressÃ£o LogÃ­stica
- ClassificaÃ§Ã£o binÃ¡ria
- Gradiente descendente
- FunÃ§Ã£o sigmoid
- NormalizaÃ§Ã£o de dados
- Teste com dataset Breast Cancer

### Decision Tree
- CritÃ©rio Gini para divisÃµes
- Controle de profundidade
- PrevenÃ§Ã£o de overfitting
- Teste com dataset Iris

### Redes Neurais
- MÃºltiplas camadas ocultas
- FunÃ§Ãµes de ativaÃ§Ã£o: Sigmoid e ReLU
- Softmax na saÃ­da
- Backpropagation
- InicializaÃ§Ã£o Xavier/He
- Testes com Iris e Digits

## ğŸ“ˆ VisualizaÃ§Ãµes Geradas

Os scripts de teste geram grÃ¡ficos automaticamente:
- Curvas de convergÃªncia
- AcurÃ¡cia vs parÃ¢metros
- Decision boundaries
- Matrizes de confusÃ£o
- DistribuiÃ§Ãµes de probabilidades
- MÃ©todo do cotovelo (K-Means)

## ğŸ’¡ Dicas de Uso

1. **NormalizaÃ§Ã£o**: Sempre normalize dados para RegressÃ£o LogÃ­stica e Redes Neurais
2. **Escolha de K**: Use validaÃ§Ã£o cruzada ou mÃ©todo do cotovelo
3. **Learning Rate**: Comece com 0.01-0.1 e ajuste
4. **Profundidade da Ãrvore**: Cuidado com overfitting, teste valores entre 3-10
5. **Arquitetura da Rede**: Comece simples e aumente se necessÃ¡rio